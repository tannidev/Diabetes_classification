{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Term Project\n",
    "##### Name: Tanni Dev, id: gp2996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project we are using \"Diabetes Health Indicators Dataset\" from Kaggle. This is a classification problem where I am trying to identify if a person has diabetics or not based on several parameters\n",
    "\n",
    "###### data link:https://www.kaggle.com/datasets/julnazz/diabetes-health-indicators-dataset/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary library and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 length 236378\n",
      "Dataset 2 length 67136\n",
      "Dataset 3 length 236378\n",
      "Data merged length 539892\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# datapath for the csv file\n",
    "file_path1 = './diabetes_health indicators_dataset/diabetes_012_health_indicators.csv'\n",
    "file_path2 = './diabetes_health indicators_dataset/diabetes_binary_5050split_health_indicators.csv'\n",
    "file_path3 = './diabetes_health indicators_dataset/diabetes_binary_health_indicators.csv'\n",
    "\n",
    "\n",
    "data1 = pd.read_csv(file_path1)\n",
    "data1_copy = data1.copy()\n",
    "data2 = pd.read_csv(file_path2)\n",
    "data3 = pd.read_csv(file_path3)\n",
    "\n",
    "data1_copy['Diabetes_012'] = data1_copy['Diabetes_012'].replace({1: 1, 2: 1, 0: 0})\n",
    "data1_copy.rename(columns={'Diabetes_012': 'Diabetes_binary'}, inplace=True)\n",
    "\n",
    "# combining all 3 datasets\n",
    "data_merged = pd.concat([data1_copy, data2, data3], ignore_index=True)\n",
    "\n",
    "print(\"Dataset 1 length\", len(data1))\n",
    "print(\"Dataset 2 length\", len(data2)) \n",
    "print(\"Dataset 3 length\", len(data3)) \n",
    "print(\"Data merged length\", len(data_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Diabetes_binary', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker',\n",
       "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
       "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
       "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
       "       'Income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if i need to pre process different types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236378 entries, 0 to 236377\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Diabetes_012          236378 non-null  float64\n",
      " 1   HighBP                236378 non-null  int64  \n",
      " 2   HighChol              236378 non-null  float64\n",
      " 3   CholCheck             236378 non-null  int64  \n",
      " 4   BMI                   236378 non-null  float64\n",
      " 5   Smoker                236378 non-null  float64\n",
      " 6   Stroke                236378 non-null  float64\n",
      " 7   HeartDiseaseorAttack  236378 non-null  float64\n",
      " 8   PhysActivity          236378 non-null  int64  \n",
      " 9   Fruits                236378 non-null  int64  \n",
      " 10  Veggies               236378 non-null  int64  \n",
      " 11  HvyAlcoholConsump     236378 non-null  int64  \n",
      " 12  AnyHealthcare         236378 non-null  int64  \n",
      " 13  NoDocbcCost           236378 non-null  float64\n",
      " 14  GenHlth               236378 non-null  float64\n",
      " 15  MentHlth              236378 non-null  float64\n",
      " 16  PhysHlth              236378 non-null  float64\n",
      " 17  DiffWalk              236378 non-null  float64\n",
      " 18  Sex                   236378 non-null  int64  \n",
      " 19  Age                   236378 non-null  int64  \n",
      " 20  Education             236378 non-null  float64\n",
      " 21  Income                236378 non-null  float64\n",
      "dtypes: float64(13), int64(9)\n",
      "memory usage: 39.7 MB\n"
     ]
    }
   ],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkin if data contains any missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:  Series([], dtype: int64)\n",
      "Dataset 2:  Series([], dtype: int64)\n",
      "Dataset 3:  Series([], dtype: int64)\n",
      "Dataset_merged:  Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Checking for any missing values in the dataset\n",
    "missing_values1 = data1.isnull().sum()\n",
    "missing_values2 = data2.isnull().sum()\n",
    "missing_values3 = data3.isnull().sum()\n",
    "missing_values_merged = data_merged.isnull().sum()\n",
    "\n",
    "missing_values_counts1 = missing_values1[missing_values1 > 0]\n",
    "missing_values_counts2 = missing_values2[missing_values2 > 0]\n",
    "missing_values_counts3 = missing_values3[missing_values3 > 0]\n",
    "missing_values_counts_merged = missing_values_merged[missing_values_merged > 0]\n",
    "\n",
    "\n",
    "print(\"Dataset 1: \", missing_values_counts1)\n",
    "print(\"Dataset 2: \", missing_values_counts2)\n",
    "print(\"Dataset 3: \", missing_values_counts3)\n",
    "print(\"Dataset_merged: \", missing_values_counts_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data in training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((377924, 21), (377924,)), ((53989, 21), (53989,)), ((107979, 21), (107979,)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "# Dataset1 with 3 class \n",
    "# X = data1.drop('Diabetes_012', axis=1)\n",
    "# y = data1['Diabetes_012']\n",
    "\n",
    "# # Dataset2 with binary class\n",
    "# X = data2.drop('Diabetes_binary', axis=1)\n",
    "# y = data2['Diabetes_binary']\n",
    "\n",
    "# # Dataset2 with binary class\n",
    "# X = data3.drop('Diabetes_binary', axis=1)\n",
    "# y = data3['Diabetes_binary']\n",
    "\n",
    "# # merged dataset with binary class\n",
    "X = data_merged.drop('Diabetes_binary', axis=1)\n",
    "y = data_merged['Diabetes_binary']\n",
    "\n",
    "\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training, validation, and test sets (70%, 10%, 20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(2/3), random_state=42)\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying 5 different ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': {'Accuracy': 0.8160551223397359, 'Precision': 0.7866369887164912, 'Recall': 0.8160551223397359, 'F1 Score': 0.7871224386053229}, 'KNN': {'Accuracy': 0.815592065050288, 'Precision': 0.8076070885475376, 'Recall': 0.815592065050288, 'F1 Score': 0.8110594535018403}, 'Bagged Decision Tree': {'Accuracy': 0.949582322324918, 'Precision': 0.9494134411606437, 'Recall': 0.949582322324918, 'F1 Score': 0.9494931910510094}, 'Random Forest': {'Accuracy': 0.9521198762710923, 'Precision': 0.9516783958428123, 'Recall': 0.9521198762710923, 'F1 Score': 0.9518527846538939}, 'XGB classifier': {'Accuracy': 0.8360036303691493, 'Precision': 0.8176976633328947, 'Recall': 0.8360036303691493, 'F1 Score': 0.8094109344999457}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, model_name, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    model_performance[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "lr_param = {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "knn_param = {'n_neighbors':10,'weights':'uniform', 'metric':'euclidean'}\n",
    "rf_param= {'n_estimators': 100 , 'max_features': 'sqrt'}\n",
    "XGB_hyper_param = {'booster': 'gbtree', 'lambda': 0.43403998704508867, 'alpha': 2.252282347254054e-06, 'max_depth': 12, 'eta': 0.03730224167231534, 'gamma': 1.6105377714235158e-07, 'subsample': 0.436607283293611, 'colsample_bytree': 0.5631258519693639}\n",
    "\n",
    "\n",
    "# List of models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(**lr_param, max_iter=1000),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Bagged Decision Tree': BaggingClassifier(n_estimators=100),\n",
    "    'Random Forest': RandomForestClassifier(**rf_param, random_state=42),\n",
    "    'XGB classifier': XGBClassifier(**XGB_hyper_param, random_state=42)   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    train_evaluate_model(model, name, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "print(model_performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, Validation and Testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model name ': 'Logistic Regression', 'Train Accuracy': 0.8176935045141351, 'Validation Accuracy': 0.8160551223397359, 'Test Accuracy': 0.8178071662082442}\n",
      "{'Model name ': 'KNN', 'Train Accuracy': 0.8941295075200305, 'Validation Accuracy': 0.815592065050288, 'Test Accuracy': 0.8188073606905046}\n",
      "{'Model name ': 'Bagged Decision Tree', 'Train Accuracy': 0.9872355288364857, 'Validation Accuracy': 0.949582322324918, 'Test Accuracy': 0.9512312579297827}\n",
      "{'Model name ': 'Random Forest', 'Train Accuracy': 0.9872434669404431, 'Validation Accuracy': 0.9521198762710923, 'Test Accuracy': 0.9538058326156011}\n",
      "{'Model name ': 'XGB classifier', 'Train Accuracy': 0.8524359395010637, 'Validation Accuracy': 0.8360036303691493, 'Test Accuracy': 0.8391631706165088}\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model on training, validation, and test sets\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # Output results\n",
    "    results = {\n",
    "        'Model name ': name, \n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Validation Accuracy': val_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "    }\n",
    "\n",
    "    print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
